parcelRequire=function(e,r,t,n){var i,o="function"==typeof parcelRequire&&parcelRequire,u="function"==typeof require&&require;function f(t,n){if(!r[t]){if(!e[t]){var i="function"==typeof parcelRequire&&parcelRequire;if(!n&&i)return i(t,!0);if(o)return o(t,!0);if(u&&"string"==typeof t)return u(t);var c=new Error("Cannot find module '"+t+"'");throw c.code="MODULE_NOT_FOUND",c}p.resolve=function(r){return e[t][1][r]||r},p.cache={};var l=r[t]=new f.Module(t);e[t][0].call(l.exports,p,l,l.exports,this)}return r[t].exports;function p(e){return f(p.resolve(e))}}f.isParcelRequire=!0,f.Module=function(e){this.id=e,this.bundle=f,this.exports={}},f.modules=e,f.cache=r,f.parent=o,f.register=function(r,t){e[r]=[function(e,r){r.exports=t},{}]};for(var c=0;c<t.length;c++)try{f(t[c])}catch(e){i||(i=e)}if(t.length){var l=f(t[t.length-1]);"object"==typeof exports&&"undefined"!=typeof module?module.exports=l:"function"==typeof define&&define.amd?define(function(){return l}):n&&(this[n]=l)}if(parcelRequire=f,i)throw i;return f}({"eAQT":[function(require,module,exports) {
"use strict";Object.defineProperty(exports,"__esModule",{value:!0}),exports.default=void 0;class e extends Error{constructor(e){super(e),this.name="VITALSPEECHTOTEXT_NODE_ERROR",this.error=e}}exports.default=e;
},{}],"hQms":[function(require,module,exports) {
"use strict";Object.defineProperty(exports,"__esModule",{value:!0}),exports.default=void 0;var t=e(require("../nodes/error.js"));function e(t){return t&&t.__esModule?t:{default:t}}class s extends EventTarget{constructor(){super(),this.status="non-emitting"}async start(e){if(this.hookedOn)throw new t.default("node ".concat(this.type," is already hooked on ").concat(this.hookedOn,", call stop() first"));if(!e)throw new t.default("".concat(this.type," requires a node argument to hook on"));if(e&&!this.hookableOnNodeTypes.includes(e.type))throw new t.default("".concat(this.type," node cannot hook on ").concat(e.type));this.startWorker(),this.hookedOn=e,this.resume()}startWorker(){this.worker&&(this.workerRuntime=this.worker.init(),this.workerRuntime.onmessage=(t=>{this.dispatchEvent(new CustomEvent(this.event,{detail:t.data}))}))}stop(){this.pause(),this.workerRuntime&&(this.workerRuntime.terminate(),delete this.workerRuntime),this.hookedOn=null}pause(){this.hookedOn&&"emitting"==this.status&&("mic"!=this.type&&this.hookedOn.removeEventListener(this.hookedOn.event,this.handler),this.status="non-emitting")}resume(){this.hookedOn&&"non-emitting"==this.status&&("mic"!=this.type&&this.hookedOn.addEventListener(this.hookedOn.event,this.handler),this.status="emitting")}}exports.default=s;
},{"../nodes/error.js":"eAQT"}],"kQf0":[function(require,module,exports) {

          module.exports.init = function() {
            const blob = new Blob(["// import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.16.1';\n\nlet pipeline_func = null;\n\n(async () => {\n  const { pipeline, env } = await import('https://cdn.jsdelivr.net/npm/@xenova/transformers@2.16.1');\n  \n    env.allowLocalModels = false;\n\n    pipeline_func = pipeline;\n    \n})();\n\nconsole.log(\"WHISPER WEB WORKER EXISTS\");\n\nonmessage = async (msg) => {\n    \n    console.log(\"WHISPER WEB WORKER: RECEIVED MESSAGE\");\n    \n    console.log(\"WHISPER WEB WORKER: event.data: \", msg.data);\n\t\n    switch (msg.data.method) {\n        case \"configure\":\n            // nativeSampleRate = msg.data.nativeSampleRate\n            // targetSampleRate = msg.data.targetSampleRate\n            // targetFrameSize = msg.data.targetFrameSize\n            // Int16Convert = msg.data.Int16Convert\n            \n            // exit after configuring\n            return Promise.resolve();\n            \n            break\n            \n        case \"process\":\n            // process(msg.data.audioFrame)\n            break        \n    }\n      \n    // processing audio for transscription...\n    \n    const message = msg.data;\n    \n    let task = message.task;\n\n    try{\n\t\t\n\t   let transcript = await transcribe(\n           message.task.recorded_audio,\n\t       message.model,\n\t       message.multilingual,\n\t       message.quantized,\n\t       message.subtask,\n\t       message.language,\n\t   );\n           \n        console.log(\"WHISPER WEB WORKER: TRANSCRIPTION RESULT: \", transcript);\n\t    \n        if (transcript === null){\n\t       console.error(\"WHISPER WEB WORKER: transcription was null\");\n\t   }\n           \n\t   if (typeof transcript === 'undefined'){\n\t       console.error(\"WHISPER WEB WORKER: transcription was undefined??\");\n\t   }\n\n        delete task.recorded_audio;\n\t\t  \n        task['transcript'] = transcript;\n\t       \n        self.postMessage({\n\t\t\t task: task,\n\t           status: \"complete\",\n\t           // task: \"automatic-speech-recognition\",\n\t           transcript: transcript,\n\t   });\n\t\t\n    }catch(e){\n\t\tconsole.error(\"ERROR: whisper worker: \", e);\n    }    \n    \n    return Promise.resolve();\n\n}\n\n// Define model factories\n// Ensures only one model is created of each type\n\nclass PipelineFactory {\n    static task = null;\n    static model = null;\n    static quantized = null;\n    static instance = null;\n\n    constructor(tokenizer, model, quantized) {\n        this.tokenizer = tokenizer;\n        this.model = model;\n        this.quantized = quantized;\n    }\n\n    static async getInstance(progress_callback = null) {\n        if (this.instance === null) {\n            this.instance = pipeline_func(this.task, this.model, {\n                quantized: this.quantized,\n                progress_callback,\n\n                // For medium models, we need to load the `no_attentions` revision to avoid running out of memory\n                revision: this.model.includes(\"/whisper-medium\") ? \"no_attentions\" : \"main\"\n            });\n        }\n\n        return this.instance;\n    }\n}\n\nclass AutomaticSpeechRecognitionPipelineFactory extends PipelineFactory {\n    static task = \"automatic-speech-recognition\";\n    static model = null;\n    static quantized = null;\n}\n\nconst transcribe = async (\n    audio,\n    model,\n    multilingual,\n    quantized,\n    subtask,\n    language,\n) => {\n\tconsole.log(\"in transcribe. audio: \", audio);\n\tconsole.log(\"whisper web worker: in transcribe.  model,multilingual,quantized,subtask,language: \", model, multilingual, quantized, subtask, language);\n    \n\tlet output = null;\n\n\ttry{\n        \n\t\tconst isDistilWhisper = model.startsWith(\"distil-whisper/\");\n\t\t\n\t    let modelName = model;\n\t    \n        if (!isDistilWhisper && !multilingual) {\n\t        modelName += \".en\"\n\t    }\n\t\t\n\t    const p = AutomaticSpeechRecognitionPipelineFactory;\n\t    \n        if (p.model !== modelName || p.quantized !== quantized) {\n\t        // Invalidate model if different\n\t        p.model = modelName;\n\t        p.quantized = quantized;\n\n\t        if (p.instance !== null) {\n\t            (await p.getInstance()).dispose();\n\t            p.instance = null;\n\t        }\n\t    }\n\t\t\n\t    // Load transcriber model\n\t    let transcriber = await p.getInstance((data) => {\n\t\t\tconsole.log(\"whisper web worker: posting something back: \", data);\n\t        self.postMessage(data);\n\t    });\n\n\t    const time_precision =\n\t        transcriber.processor.feature_extractor.config.chunk_length /\n\t        transcriber.model.config.max_source_positions;\n\n\t    // Storage for chunks to be processed. Initialise with an empty chunk.\n\t    let chunks_to_process = [\n\t        {\n\t            tokens: [],\n\t            finalised: false,\n\t        },\n\t    ];\n\n\t    // TODO: Storage for fully-processed and merged chunks\n\t    // let decoded_chunks = [];\n\t\t\n\t    function chunk_callback(chunk) {\n\t\t\tconsole.log(\"in whisper chunk callback. chunk: \", chunk);\n\t        let last = chunks_to_process[chunks_to_process.length - 1];\n\n\t        // Overwrite last chunk with new info\n\t        Object.assign(last, chunk);\n\t        last.finalised = true;\n\n\t        // Create an empty chunk after, if it not the last chunk\n\t        if (!chunk.is_last) {\n\t            chunks_to_process.push({\n\t                tokens: [],\n\t                finalised: false,\n\t            });\n\t        }\n\t    }\n\t\t\n\t    // Inject custom callback function to handle merging of chunks\n\t    function callback_function(item) {\n\t\t\t//console.log(\"whisper_worker: COMPLETE?  item: \", item);\n\t        let last = chunks_to_process[chunks_to_process.length - 1];\n\n\t        // Update tokens of last chunk\n\t        last.tokens = [...item[0].output_token_ids];\n\n\t        // Merge text chunks\n\t        // TODO optimise so we don't have to decode all chunks every time\n\t        let data = transcriber.tokenizer._decode_asr(chunks_to_process, {\n\t            time_precision: time_precision,\n\t            return_timestamps: true,\n\t            force_full_sequences: false,\n\t        });\n\n\t        self.postMessage({\n\t            status: \"update\",\n\t            // task: \"automatic-speech-recognition\",\n\t            data: data,\n\t        });\n\t    }\n\t\t\n\t    // Actually run transcription\n\t    output = await transcriber(audio, {\n\t\t\t\n\t        // Greedy\n\t        top_k: 0,\n\t        do_sample: false,\n\n\t        // Sliding window\n\t        chunk_length_s: isDistilWhisper ? 20 : 30,\n\t        stride_length_s: isDistilWhisper ? 3 : 5,\n\n\t        // Language and task\n\t        language: language,\n\t        task: subtask,\n\n\t        // Return timestamps\n\t        return_timestamps: true,\n\t        force_full_sequences: false,\n\n\t        // Callback functions\n\t        callback_function: callback_function, // after each generation step\n\t        chunk_callback: chunk_callback, // after each chunk is processed\n\t    })\n\n\t\t.catch((error) => {\n\t\t\tconsole.error(\"ERROR, actually running whisper failed\");\n\n\t        return null;\n\t    });\n\t\t\n\t\tconsole.log(\"beyond WHISPER transcribe. output: \", output);\n\t\t\n\t}\n\tcatch(e){\n\t\tconsole.error(\"Whisper worker: error in transcribe function: \", e);\n\t}\n\t\n\n    return output;\n};\n\n/*\nfunction process(audioFrame) {\n    for (let sample of audioFrame)\n    {\n        //binary 111111111111111, casts to 16Bit wav file spec\n        Int16Convert ? inputBuffer.push(sample * 32767) : inputBuffer.push(sample)\n    }\n    while ((inputBuffer.length * targetSampleRate / nativeSampleRate) > targetFrameSize) {\n        let outputFrame\n        Int16Convert ? outputFrame = new Int16Array(targetFrameSize) : outputFrame = new Float32Array(targetFrameSize)\n        let sum = 0\n        let num = 0\n        let outputIndex = 0\n        let inputIndex = 0\n        while (outputIndex < targetFrameSize) {\n            sum = 0\n            num = 0\n            while (inputIndex < Math.min(inputBuffer.length, (outputIndex + 1) * nativeSampleRate / targetSampleRate)) {\n                sum += inputBuffer[inputIndex]\n                num++\n                inputIndex++\n            }\n            outputFrame[outputIndex] = sum / num\n            outputIndex++\n        }\n        inputBuffer = inputBuffer.slice(inputIndex)\n        postMessage(outputFrame)\n    }\n}\n*/\n"], { type: 'text/javascript' });
            const workerUrl = URL.createObjectURL(blob);
            return new Worker(workerUrl) 
          };
        
},{}],"KJVm":[function(require,module,exports) {
"use strict";Object.defineProperty(exports,"__esModule",{value:!0}),exports.default=void 0;var e=r(require("../nodes/node.js")),t=r(require("../workers/speechtotext.blob.js")),s=r(require("../nodes/error.js"));function r(e){return e&&e.__esModule?e:{default:e}}const o=function(e){console.log("in do_whisper_web. event: ",e);let t=e.detail;if(console.log("in do_whisper_web. task: ",t),this.whisper_worker_busy)return void console.log("do_whisper_web was called while whisper worker was busy. Aborting.");if(void 0===t.recorded_audio)return void console.log("do_whisper_web: task did not contain recorded_audio. Aborting.");t.state="stt_in_progress";let s=!1;console.log("do_whisper_web: sending audio to whisper worker: ",t.recorded_audio),this.workerRuntime.postMessage({task:t,model:"Xenova/whisper-tiny",multilingual:s,quantized:!1,subtask:null,language:s?void 0:null})};class i extends e.default{constructor(){super(),this.worker=t.default,this.handler=o.bind(this),this.type="speechToText",this.event="speechToTextFrame",this.hookableOnNodeTypes=[],this.hookedOn=null,this.whisper_worker_busy=!1,this.whisper_worker_error_count=0}async start(){return this.startWorker(),this.resume(),this.hookedOn=!0,"non-emitting"==this.status&&this.hookedOn&&(this.workerRuntime.postMessage({method:"configure"}),this.workerRuntime.addEventListener("message",e=>{if("string"==typeof e.data.status)if("progress"==e.data.status);else if("ready"==e.data.status)console.log("whisper worker sent ready message"),this.whisper_worker_busy=!1;else if("initiate"==e.data.status)console.log("whisper worker sent initiate message");else if("download"==e.data.status)console.log("whisper worker sent download message");else if("update"==e.data.status)"object"==typeof e.data.data&&null!=e.data.data&&e.data.data.length;else if("complete"==e.data.status)if(this.whisper_worker_busy=!1,console.log("GOT WHISPER COMPLETE.  e.data: ",e.data),console.log("GOT WHISPER COMPLETE.  e.data.transcript: ",e.data.transcript),console.log("GOT WHISPER COMPLETE.  e.data.task: ",e.data.task),null==e.data.transcript)console.warn("whisper recognition failed. If this is the first run, that's normal.");else if(void 0!==e.data.transcript){if(console.log("whisper returned transcription text: ",e.data.transcript),Array.isArray(e.data.transcript))console.log("typeof transcription is array");else if("object"==typeof e.data.transcript&&"string"==typeof e.data.transcript.text){let t=e.data.transcript.text;console.log("GOT TEXT: ",t);const s=new CustomEvent("speechToTextResult",{detail:t});window.dispatchEvent(s)}}else console.log("transcript was not in whisper e.data");else console.log("whisper worker sent a content message"),this.whisper_worker_busy=!1,null==e.data.data&&console.warn("whisper recognition failed. If this is the first run, that's normal.")}),this.workerRuntime.addEventListener("error",e=>{console.error("ERROR: whisper_worker sent error. terminating!. Error was: ",e,e.message),this.whisper_worker_error_count++,this.whisper_worker.terminate(),this.whisper_worker_busy=!1,void 0!==e&&this.whisper_worker_error_count<10?setTimeout(()=>{console.log("calling restart whisper worker"),restart(node)},1e3):console.error("whisper_worker errored out")}),this.status="emitting"),Promise.resolve()}async restart(e){}}exports.default=i;
},{"../nodes/node.js":"hQms","../workers/speechtotext.blob.js":"kQf0","../nodes/error.js":"eAQT"}],"X5WU":[function(require,module,exports) {
"use strict";var e=t(require("./vitalspeechtotext/nodes/speechtotext.js"));function t(e){return e&&e.__esModule?e:{default:e}}class s{constructor(){this.speechToText=new e.default}async start(){await this.speechToText.start()}async stop(){await this.speechToText.stop()}async restart(){console.log("attempting to restart whisper worker...")}do_whisper_web(e){arguments.length>1&&void 0!==arguments[1]&&arguments[1];console.log("in do_whisper_web. task: ",e),this.speechToText.handler(new CustomEvent("speechToText",{detail:e}))}}window.VitalSpeechToText=s;
},{"./vitalspeechtotext/nodes/speechtotext.js":"KJVm"}]},{},["X5WU"], null)